import numpy as np 

class NeuralNetwork:
	"""
		General Neural Network implementation
	"""

	def __init__(self,shape,activation_function="sigmoid"):
		"""Initializes a neural network model

		Parameters
		----------

		shape: list
			A list of integers specifying number of nodes on each layer starting from input to output layer
		activation: string
			The activation function to be used. Currently only sigmoid is implemented
		"""
		self.shape = shape	# list storing the structure of the network
		self.activation_function = activation_function	# activation function to be used
		
		self.weights = list() # list of weights per each layer
		self.neurons = list() #list of neurons per each layer

		for i in range(1,len(shape)):
			# creation of parameter matrices for each layer mapping with random values
			# size of weight matrix for each layer => (num of units in next layer * num of units in that layer + 1 (for bias))
			self.weights.append(np.random.random_sample((shape[i],shape[i-1]+1)))

		

	def forward_propagation(self,input):
		""" Calculates the neuron's values based on input and current set of weights
		
		Parameters
		----------

		X: list
			A list with input values for input layer
		"""

		#Create numpy array from list and check if input value are compatible with input layer
		input = np.array(input)
		if(np.shape(input)[0] != self.shape[0]):
			raise ValueError("Input array size not compatible with input layer size")

		# first layer's activation is the input itself, but first transposing ndarray to make a column vector
		transposed_input = input.reshape((-1,1))
		
		# adding 1 on top for bias unit
		transposed_input_bias = np.vstack((np.array([1]),transposed_input))

		# first neurons are input with a bias on top
		self.neurons.append(transposed_input_bias)	

		for i in range(1,len(self.shape)):

			# get the activation function selected
			activation = self.activation()

			activation_result = activation(np.dot(self.weights[i-1],self.neurons[i-1]))

			# we need to add bias for all layer except the output layer
			if i != len(self.shape)-1:
				activation_result_bias = np.vstack((np.array([1]),activation_result))
				self.neurons.append(activation_result_bias)
			else:
				self.neurons.append(activation_result)
		
		# Sanity check for checking the size of all neuron matrices of all the layers
		# size of neuron matrix for each layer => (num of units in this layer + 1 (for bias) * 1)
		for i in range(0,len(self.neurons)):
			print(np.shape(self.neurons[i]))

		#return the output vector generated by output layer
		return self.neurons[len(self.neurons)-1]

	def back_propagation(self,y):

		delta = list()	# store the errors
		gradient = list()	#store the gradient values
		last = len(self.neurons)-1
		for k in range(0,m):
			j=1
			delta.append(self.neurons[last] - y[k])	
			# looping from the last layer to second layer (reverse)
			for i in range(len(self.neurons),1,-1):
				d = np.multiply(np.dot(self.neurons[].transpose(),delta[-j]),np.multiply(self.neurons[i],(1 - self.neurons[i])))
				delta.append(i)
				j++





	def activation(self):
		""" Returns the activation function given in the constructor parameter

		Returns
		-------
		function: function
			The activation function
		"""
		def sigmoid(val):
			return float(1)/(1+np.exp(-val))
		
		if(self.activation_function == "sigmoid"):
			return sigmoid

NN = NeuralNetwork([2,2,2,1])
print(NN.forward_propagation([0.3,0.5]))